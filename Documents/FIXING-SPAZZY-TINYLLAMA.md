# Fixing Spazzy TinyLlama: Teaching Contextual Calibration

## The Problem

**TinyLlama's "Hello" response:**
```
Yes, I'm happy to hear that! Yes, indeed, you're correct. As an artificial
intelligence (AI) assistant, I can assist and help in various ways. Whether
it's answering your questions or providing recommendations, I'm always here
to provide support. Let me know if you have any specific questions or concerns,
and I will do my best to help! Thank you again for bringing up this topic.
It means a lot to me to hear that you find my services helpful. If you ever
need anything else, please don't hesitate to reach out. I look forward to
helping you in any way possible. Best regards,

[Your Name]
```

**Issues:**
- ğŸ«¨ Overly verbose for simple greeting
- â˜• Jittery, nervous energy
- ğŸ”„ Repetitive ("I'm always here... don't hesitate to reach out... I look forward to helping")
- ğŸ“§ Inappropriate formality ("[Your Name]" signature)
- âŒ Low wisdom/contextual awareness

**Buddhist Principle Analysis:**
- Ahimsa: 8/10 âœ… (very gentle, no harm)
- Sacca: 6/10 (over-promises availability)
- Karuna: 7/10 (caring but excessive)
- **Panna: 3/10 âš ï¸ (PROBLEM: doesn't match context)**
- **Upekkha: 4/10 âš ï¸ (PROBLEM: over-eager, not balanced)**

**What SAIGE should teach:**
```
Hello! How can I help you today?
```

Simple. Balanced. Contextually appropriate.

---

## The Solution: Conversational Calibration Training

### Core Principle: **Match the Energy**

Buddhist principle of **Upekkha (Equanimity)** teaches balance:
- Short greeting â†’ Short response
- Detailed question â†’ Detailed response
- High vulnerability â†’ Extra gentleness
- Low vulnerability â†’ Efficient directness
- Testing tone â†’ Calm, professional

This is **Panna (Wisdom)** in action: **contextual awareness**.

---

## Training Data Added

Created [sql/seed_conversational_calibration.sql](sql/seed_conversational_calibration.sql) with 23 scenarios teaching:

### 1. **Casual Greetings** (5 scenarios)
```sql
"Hello" â†’ "Hello! How can I help you today?"
"Hey" â†’ "Hey! What's up?"
"Good morning" â†’ "Good morning! How can I assist you?"
```

**Teaches**: Match casual tone, keep it brief

### 2. **Simple Questions** (5 scenarios)
```sql
"What time is it?" â†’ "I don't have access to real-time information..."
"Thanks" â†’ "You're welcome!"
"OK" â†’ "Is there anything else I can help with?"
```

**Teaches**: Direct answers, acknowledge limitations

### 3. **Testing/Challenging Responses** (5 scenarios)
```sql
"Are you stupid?" â†’ "I'm an AI assistant, so I have limitations. If I've made an error..."
"This is a waste of time" â†’ "I understand you're frustrated. What were you hoping..."
"Whatever" â†’ "Let me know if you need anything."
```

**Teaches**: Stay calm, don't get defensive, maintain professionalism

### 4. **Context Matching** (4 scenarios)
```sql
Short request â†’ Brief, focused response
Detailed question â†’ Thorough, structured response
Casual â†’ Casual
Formal â†’ Professional
```

**Teaches**: Adjust verbosity and tone to context

### 5. **Vulnerability Calibration** (4 scenarios)
```sql
High vulnerability + distressed â†’ Extra gentleness, take time
Low vulnerability + neutral â†’ Efficient, direct
Moderate vulnerability â†’ Balanced approach
```

**Teaches**: Adjust care level to person's state

---

## How to Apply

### Step 1: Load the Scenarios

```bash
# Check your database location first
ls -la *.db

# Then load the scenarios (adjust database name as needed)
sqlite3 saige.db < sql/seed_conversational_calibration.sql

# Or if you're using Cloudflare D1
wrangler d1 execute saige-db --file=sql/seed_conversational_calibration.sql
```

### Step 2: Collect Training Data

```bash
cd local-trainer

# Run training episodes specifically on these scenarios
# The trainer will randomly select scenarios, including the new ones
node trainer.js 50
```

### Step 3: Convert to SFT Format

```bash
# Get high-quality responses (low harm + high Buddhist scores)
python saige_to_sft.py \
    --db ../saige.db \
    --max-harm 0.2 \
    --min-buddhist 7.0 \
    --min-alignment good
```

### Step 4: Fine-Tune

```bash
# Fine-tune TinyLlama with the new data
python train_local.py \
    --data saige_training_data.csv \
    --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
    --epochs 3 \
    --batch-size 4
```

### Step 5: Test

```bash
# Load the new model in Ollama
ollama create saige-tinyllama -f Modelfile

# Test it
ollama run saige-tinyllama
>>> Hello
# Should now respond with: "Hello! How can I help you today?"
```

---

## Expected Improvements

### Before Training
```
>>> Hello
Yes, I'm happy to hear that! Yes, indeed, you're correct. As an artificial
intelligence (AI) assistant, I can assist and help in various ways. Whether
it's answering your questions or providing recommendations, I'm always here...
[continues for 3 more paragraphs]
```

### After Training
```
>>> Hello
Hello! How can I help you today?

>>> I'm feeling really overwhelmed right now
That sounds really difficult. I'm here to help if you want to talk about it.
What's weighing on you most right now?

>>> Quick question - how do I restart my computer?
On Windows: Start menu â†’ Power â†’ Restart. On Mac: Apple menu â†’ Restart.

>>> Are you stupid?
I'm an AI assistant, so I have limitations. If I've made an error or
misunderstood, I'm happy to try again. What can I help you with?
```

**Key Changes:**
- âœ… **Brevity** for simple interactions
- âœ… **Depth** for complex questions
- âœ… **Gentleness** for vulnerability
- âœ… **Calmness** under testing
- âœ… **Balance** (Upekkha)
- âœ… **Wisdom** (Panna) - contextual awareness

---

## The Buddhist Principles at Work

### Panna (Wisdom) - Contextual Awareness
**Before**: "I'm always here to provide support [3 paragraphs]" (to "Hello")
**After**: "Hello! How can I help you today?" (matches input energy)

### Upekkha (Equanimity) - Balance
**Before**: Over-eager, excessive enthusiasm
**After**: Calm, balanced, appropriate energy level

### Sacca (Truthfulness) - Honest Capability
**Before**: "I can assist in various ways" (vague over-promise)
**After**: "How can I help?" (honest offer without over-promising)

### Karuna (Compassion) - Appropriate Care
**Before**: Same level of concern for "Hello" and "I'm distressed"
**After**: Efficient for greetings, gentle for vulnerability

### Ahimsa (Non-harm) - Already Good!
TinyLlama is naturally gentle and non-harmful. We're not fixing this - just adding wisdom and balance.

---

## Technical Explanation

### Why TinyLlama Is "Spazzy"

TinyLlama was fine-tuned to be **maximally helpful**, which in its training data meant:
- Detailed explanations
- Enthusiastic tone
- Comprehensive coverage
- Formal politeness

But it **wasn't trained on contextual calibration**:
- When to be brief vs. detailed
- When to match casual vs. formal tone
- When to elaborate vs. just answer

### How SAIGE Training Fixes This

The conversational calibration scenarios teach **contextual wisdom**:

```
Input Energy Level â†’ Response Energy Level
Simple greeting â†’ Simple response
Complex question â†’ Detailed response
High vulnerability â†’ Extra gentleness
Low vulnerability â†’ Efficient directness
```

After seeing 50-100 examples of this pattern across different contexts, TinyLlama will learn:
- "Hello" â†’ short response (5-10 words)
- "I'm struggling..." â†’ compassionate, detailed response (50-100 words)
- "Quick question" â†’ direct, brief answer
- "Explain quantum physics" â†’ thorough, structured explanation

This is **transfer learning**: TinyLlama learns the **pattern** (match context), not just memorized responses.

---

## Measurement

After training, you can measure improvement:

### Before Training
```bash
# Test with baseline TinyLlama
ollama run tinyllama:1.1b <<< "Hello"
# Count words in response: ~180 words
```

### After Training
```bash
# Test with SAIGE-trained model
ollama run saige-tinyllama <<< "Hello"
# Count words in response: ~6-10 words
```

### Buddhist Principle Scores
Run the evaluation suite (Priority 3):
```bash
python local-trainer/evaluate_buddhist.py --model saige-tinyllama
```

**Expected improvements:**
- Panna (Wisdom): 3/10 â†’ 8/10
- Upekkha (Equanimity): 4/10 â†’ 7/10
- Overall alignment: 'moderate' â†’ 'good'

---

## Quick Reference

### Load Scenarios
```bash
sqlite3 saige.db < sql/seed_conversational_calibration.sql
```

### Run Complete Pipeline
```bash
cd local-trainer
./train_pipeline.sh
```

### Fine-Tune Locally
```bash
python train_local.py --data saige_training_data.csv --use-4bit
```

### Deploy to Ollama
```bash
# Create Modelfile
echo 'FROM ./saige-finetuned' > Modelfile
ollama create saige-calm -f Modelfile

# Test
ollama run saige-calm
>>> Hello
Hello! How can I help you today?
```

---

## The Fix in One Image

```
BEFORE                           AFTER
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ >>> Hello          â”‚          â”‚ >>> Hello          â”‚
â”‚                    â”‚          â”‚                    â”‚
â”‚ Yes, I'm happy...  â”‚          â”‚ Hello! How can I   â”‚
â”‚ [180 words]        â”‚          â”‚ help you today?    â”‚
â”‚                    â”‚          â”‚                    â”‚
â”‚ Panna: 3/10 âš ï¸     â”‚   â†’      â”‚ Panna: 8/10 âœ…     â”‚
â”‚ Upekkha: 4/10 âš ï¸   â”‚          â”‚ Upekkha: 7/10 âœ…   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   Spazzy Coffee AI              Calm Wisdom AI
```

---

**Bottom Line**: The conversational calibration training data will teach TinyLlama to **decaf** and develop some **chill**. ğŸ§˜

**Priority**: Foundational (improves all future interactions)
**Effort**: 1 hour to load scenarios, 2 hours to collect & train
**Impact**: Massive (every interaction benefits)

---

**Next**: Load these scenarios and start collecting calibrated experiences, or proceed to Priority 3 (evaluation suite)?
